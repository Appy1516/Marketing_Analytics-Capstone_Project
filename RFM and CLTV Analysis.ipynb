{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Loading modules\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def rfm():\n",
    "    # Ensure that all columns are shown \n",
    "    pd.set_option('display.max_columns', None) \n",
    "    df = pd.read_excel(\"../input/turkish-market-sales-dataset-with-9000items/MarketSales.xlsx\")\n",
    "    df.head()\n",
    "\n",
    "    # Returns the individual number of dates\n",
    "    df[\"STARTDATE\"].nunique()\n",
    "\n",
    "    # Returns missing values\n",
    "    df.isnull().sum()\n",
    "\n",
    "    # query of data size\n",
    "    df.shape\n",
    "\n",
    "    # number of unexpected products\n",
    "    df.CATEGORY_NAME1.nunique()\n",
    "    df.CATEGORY_NAME2.nunique()\n",
    "    df.CATEGORY_NAME3.nunique()\n",
    "\n",
    "    # There are 354 products without any esthetics(beauty products).\n",
    "    df.BRAND.nunique()\n",
    "\n",
    "    # shows how many times the products have gone\n",
    "    df.CATEGORY_NAME3.value_counts().head(30)\n",
    "\n",
    "    # Which product is the most siparised?\n",
    "    # negative value due to cancellations\n",
    "    df.groupby(\"CATEGORY_NAME3\") .agg({\"AMOUNT\":\"sum\"}).head()\n",
    "\n",
    "    # Which product was purchased the most.\n",
    "    df.groupby(\"CATEGORY_NAME3\").agg({\"AMOUNT\":\"sum\"}).sort_values(\"AMOUNT\" ,ascending = False)\n",
    "\n",
    "    # Total Price\n",
    "    df[\"TotalPrice\"] = df[\"AMOUNT\"] * df[\"PRICE\"]\n",
    "    df.head()\n",
    "\n",
    "    # According to the invoice, the most likely \n",
    "    df.groupby(\"FICHENO\").agg({\"TotalPrice\":\"sum\"}).head()\n",
    "\n",
    "    # the city with the most shoppers\n",
    "    df.CITY.value_counts()[0:5]\n",
    "\n",
    "    # The city that spends the most money\n",
    "    df.groupby(\"CITY\").agg({\"TotalPrice\":\"sum\"}).sort_values(\"TotalPrice\" , ascending = False).head()\n",
    "\n",
    "    # Show quarters\n",
    "    df.describe([0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99]).T\n",
    "\n",
    "    # It's a show of outliers.\n",
    "    for feature in [\"AMOUNT\",\"PRICE\",\"TotalPrice\"]:\n",
    "\n",
    "        Q1 = df[feature].quantile(0.01)\n",
    "        Q3 = df[feature].quantile(0.99)\n",
    "        IQR = Q3-Q1\n",
    "        upper = Q3 + 1.5*IQR\n",
    "        lower = Q1 - 1.5*IQR\n",
    "\n",
    "        if df[(df[feature] > upper) | (df[feature] < lower)].any(axis=None):\n",
    "            print(feature,\"yes\")\n",
    "            print(df[(df[feature] > upper) | (df[feature] < lower)].shape[0])\n",
    "        else:\n",
    "            print(feature, \"no\")\n",
    "\n",
    "\n",
    "    # Recency - Recent date\n",
    "    # Last purchased date.\n",
    "    df[\"STARTDATE\"].max()\n",
    "\n",
    "    # I've added the last date of the day as the date of the day\n",
    "    import datetime as dt\n",
    "    today_date = dt.datetime(2017 , 11 ,16)\n",
    "    today_date\n",
    "\n",
    "    # We're looking at the last time when the clients shop.\n",
    "    df.groupby(\"CLIENTCODE\").agg({\"STARTDATE\":\"max\"}).head()\n",
    "\n",
    "    # We're retiring from the last time the customers shopd.\n",
    "    temp_df = (today_date - df.groupby(\"CLIENTCODE\").agg({\"STARTDATE\":\"max\"}))\n",
    "\n",
    "    # changing name \n",
    "    temp_df.rename(columns = {\"STARTDATE\":\"Recency\"} , inplace= True)\n",
    "\n",
    "    # We only got the number of days.\n",
    "    recency_df = temp_df.Recency.apply(lambda x : x.days)\n",
    "\n",
    "    # Frequency (sıklık)\n",
    "    freq_df = df.groupby(\"CLIENTCODE\").agg({\"FICHENO\":\"nunique\"})\n",
    "\n",
    "    # changing name\n",
    "    freq_df.rename(columns = {\"FICHENO\" : \"Frequency\"} , inplace = True)\n",
    "\n",
    "    # Monetary (the total amount of money left by the client)\n",
    "    monetary_df = df.groupby(\"CLIENTCODE\").agg({\"TotalPrice\":\"sum\"})\n",
    "    monetary_df.head()\n",
    "\n",
    "    # Changing name\n",
    "    monetary_df.rename(columns= {\"TotalPrice\":\"Monetary\"} , inplace = True)\n",
    "\n",
    "    # merge the dfs\n",
    "    rfm = pd.concat([recency_df , freq_df , monetary_df] , axis = 1)\n",
    "\n",
    "    # Scoring process\n",
    "    rfm[\"RecencyScore\"] = pd.qcut(rfm[\"Recency\"], 5, labels = [5, 4 , 3, 2, 1])\n",
    "    rfm.head()\n",
    "    rfm[\"Recency\"].min()\n",
    "    rfm[\"Frequency\"].min()\n",
    "\n",
    "    # Scoring process\n",
    "    rfm[\"FrequencyScore\"]= pd.qcut(rfm[\"Frequency\"].rank(method=\"first\"),5, labels=[1,2,3,4,5])\n",
    "\n",
    "    # Scoring process\n",
    "    rfm[\"MonetaryScore\"] = pd.qcut(rfm[\"Monetary\"] , 5 , labels = [1,2,3,4,5])\n",
    "\n",
    "    rfm = rfm.dropna()\n",
    "\n",
    "    # RFM scores are equipped with categorical value\n",
    "    rfm[\"rfm_score\"] = (rfm.RecencyScore.astype(str)+\n",
    "                        rfm.FrequencyScore.astype(str) +\n",
    "                        rfm.MonetaryScore.astype(str))\n",
    "\n",
    "    # Bringing in the people who place the most orders and are active on the site.\n",
    "    rfm.loc[rfm.rfm_score==\"555\"]\n",
    "\n",
    "    # Regular Expressions; RFM map was mapped using\n",
    "    seg_map = {\n",
    "        r'[1-2][1-2]': 'Hibernating',\n",
    "        r'[1-2][3-4]': 'At Risk',\n",
    "        r'[1-2]5': 'Can\\'t Loose',\n",
    "        r'3[1-2]': 'About to Sleep',\n",
    "        r'33': 'Need Attention',\n",
    "        r'[3-4][4-5]': 'Loyal Customers',\n",
    "        r'41': 'Promising',\n",
    "        r'51': 'New Customers',\n",
    "        r'[4-5][2-3]': 'Potential Loyalists',\n",
    "        r'5[4-5]': 'Champions'\n",
    "    }\n",
    "\n",
    "    # Monetary value is excluded ,  FrequencyScore vs RecencyScore degerlerinin birlestirilerek segment isminde degisken olusturulması.\n",
    "    rfm[\"segment\"] =rfm.RecencyScore.astype(str) + rfm.FrequencyScore.astype(str)\n",
    "    rfm.head() \n",
    "\n",
    "    # I named it using regex by applying it to the all segment..\n",
    "    rfm.segment = rfm.segment.replace(seg_map , regex = True)\n",
    "\n",
    "    # df and rfm\n",
    "    result = pd.merge(df, rfm, on='CLIENTCODE')\n",
    "    \n",
    "    # Median value of total expenditure of groups according to segment.\n",
    "    # result.groupby(\"segment\").agg({\"TotalPrice\":np.median})\n",
    "\n",
    "    # Recency of segments , Average and media scrutiny according to frequency and monetary values.\n",
    "    #rfm[[\"segment\",\"Recency\",\"Frequency\",\"Monetary\"]].groupby(\"segment\").agg([\"mean\",\"median\",\"count\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis on all columns\n",
    "\n",
    "def normal( support , threshold):\n",
    "    \"\"\"\n",
    "    support = 'min_support' value used in bivariate analysis\n",
    "    threshold = The value of the \"association_rules\" function used in Bivariate analysis.\n",
    "    \"\"\"\n",
    "    # loading libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "   \n",
    "    # Identification of data using function\n",
    "    def satislar_load():\n",
    "        dff =pd.read_excel(\"../input/turkish-market-sales-dataset-with-9000items/MarketSales.xlsx\")\n",
    "        return dff\n",
    "    \n",
    "    # Assigning a data set\n",
    "    df = satislar_load()\n",
    "    \n",
    "    # Selecting one of the cities in the data\n",
    "    df = df.loc[df.BRANCH == \"İstanbul Subesi\"]\n",
    "\n",
    "\n",
    "    # Dividing the date part of the data into days, months, years\n",
    "    df[\"STARTDATE\"] = df[\"STARTDATE\"].astype('datetime64[ns]')\n",
    "    df[\"year\"] =df[\"STARTDATE\"].dt.year\n",
    "    df[\"month\"] =df[\"STARTDATE\"].dt.month\n",
    "    df[\"day\"] =df[\"STARTDATE\"].dt.day\n",
    "    df[\"day_name\"]= df.STARTDATE.dt.day_name()\n",
    "\n",
    "    # backup of df\n",
    "    df_yedek = df.copy()\n",
    "\n",
    "    # Taking the individual names of the products\n",
    "    df_urunler = df_yedek.CATEGORY_NAME3.unique()\n",
    "\n",
    "    # sale of products according to all times\n",
    "    # for a in df_urunler:\n",
    "        #for i in [\"day\"  ,\"day_name\"]:\n",
    "            #sns.countplot(df_yedek.loc[df_yedek.CATEGORY_NAME3 == a, \"CATEGORY_NAME3\"], hue=df_yedek[i])\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "    ### Bivariate Analysis\n",
    "    df_genel = df_yedek.copy()\n",
    "     \n",
    "    # Separation of products\n",
    "    df_yedek = df_yedek.CATEGORY_NAME3.str.strip(\",\")\n",
    "    \n",
    "    # Data merge\n",
    "    dff = pd.concat([df_genel.FICHENO , df_yedek] , axis = 1 )\n",
    "    \n",
    "    # Evaluation of missing value \n",
    "    dff = dff.dropna()\n",
    "\n",
    "    # Combination of products taken together on each issued invoice with a single line by ,\n",
    "    dff = dff.groupby('FICHENO')['CATEGORY_NAME3'].agg(','.join).reset_index()\n",
    "    \n",
    "    # Dropping FICHENO \n",
    "    dff = dff.drop(\"FICHENO\" , axis = 1)\n",
    "    \n",
    "    # Separating products\n",
    "    data = list(dff['CATEGORY_NAME3'].apply(lambda x:x.split(\",\")))\n",
    "    \n",
    "\n",
    "    # Encoding processing\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    tencoder = TransactionEncoder()\n",
    "    te_data = tencoder.fit(data).transform(data)\n",
    "    df = pd.DataFrame(te_data, columns=tencoder.columns_)\n",
    "   \n",
    "\n",
    "    # Using the Apriori algorithm - uses frequent itemsets to generate association\n",
    "    # used for working on trasactional datasets\n",
    "    # with association rule it determines how strongly or weakly 2 objects are connected\n",
    "    from mlxtend.frequent_patterns import apriori,  association_rules\n",
    "    df1 = apriori(df, min_support=support, use_colnames=True)\n",
    "   \n",
    "    # Scrapping the rules of bivariate\n",
    "    df_association = association_rules(df1, metric = 'confidence', min_threshold =threshold)\n",
    "    \n",
    "    return df_association.sort_values(by='confidence', ascending=False).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal( 0.01 , 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION IN DAYS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION IN DAYS\n",
    "def day( support , threshold , day , plot = False ):\n",
    "    \"\"\"\n",
    "    support = 'min_support' value used in bivariate analysis\n",
    "    threshold = The value of the \"association_rules\" function used in Bivariate analysis.\n",
    "    day = Applies bivariate analysis on a day-to-day basis to be selected\n",
    "    plot = Returns gun-based graphics of products when \"True\" is made.\n",
    "    \"\"\"\n",
    "    # Loading Libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Identification of data to the function\n",
    "    def satislar_load():\n",
    "        dff =pd.read_excel(\"../input/turkish-market-sales-dataset-with-9000items/MarketSales.xlsx\")\n",
    "        return dff\n",
    "    \n",
    "    # Assigning a dataset\n",
    "    df = satislar_load()\n",
    "    \n",
    "    # Selecting one of the cities in the data\n",
    "    df = df.loc[df.BRANCH == \"İstanbul Subesi\"]\n",
    "\n",
    "\n",
    "    ## ucuncu. adım\n",
    "\n",
    "    # Dividing the date part of the data into days, months, years\n",
    "    df[\"STARTDATE\"] = df[\"STARTDATE\"].astype('datetime64[ns]')\n",
    "    df[\"year\"] =df[\"STARTDATE\"].dt.year\n",
    "    df[\"month\"] =df[\"STARTDATE\"].dt.month\n",
    "    df[\"day\"] =df[\"STARTDATE\"].dt.day\n",
    "    df[\"day_name\"]= df.STARTDATE.dt.day_name()\n",
    "\n",
    "\n",
    "    # backup of df  \n",
    "    df_yedek = df.copy()\n",
    "    \n",
    "    # days selected  \n",
    "    df_yedek = df_yedek.loc[df_yedek.day_name == day]\n",
    "    \n",
    "    # Taking the individual names of the products\n",
    "    df_urunler = df_yedek.CATEGORY_NAME3.unique()\n",
    "\n",
    "    # sale of products according to all times\n",
    "    \n",
    "    if plot:\n",
    "        for a in df_urunler:\n",
    "            for i in [\"hour\" ,\"day_name\"]:\n",
    "                sns.countplot(df_yedek.loc[df_yedek.CATEGORY_NAME3 == a, \"CATEGORY_NAME3\"], hue=df_yedek[i])\n",
    "                plt.show()\n",
    "\n",
    "    ### Bivariate Analysis\n",
    "    df_genel = df_yedek.copy()\n",
    "    \n",
    "    # Separation of products with ;\n",
    "    df_yedek = df_yedek.CATEGORY_NAME3.str.strip(\",\")\n",
    "\n",
    "    # Data merge\n",
    "    dff = pd.concat([df_genel.FICHENO , df_yedek] , axis = 1 )\n",
    "    \n",
    "    # Evaluation of missing value\n",
    "    dff = dff.dropna()\n",
    "\n",
    "    # Combination of products taken together on each issued invoice with a single line \n",
    "    dff = dff.groupby('FICHENO')['CATEGORY_NAME3'].agg(','.join).reset_index()\n",
    "\n",
    "    # Drop FICHENO \n",
    "    dff = dff.drop(\"FICHENO\" , axis = 1)\n",
    "    \n",
    "    # separation of products\n",
    "    data = list(dff['CATEGORY_NAME3'].apply(lambda x:x.split(\",\")))\n",
    "    \n",
    "\n",
    "    # Encoding processing\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    tencoder = TransactionEncoder()\n",
    "    te_data = tencoder.fit(data).transform(data)\n",
    "    df = pd.DataFrame(te_data, columns=tencoder.columns_)\n",
    "   \n",
    "\n",
    "    # Using the Apriori algorithm\n",
    "    from mlxtend.frequent_patterns import apriori,  association_rules\n",
    "    df1 = apriori(df, min_support=support, use_colnames=True)\n",
    "   \n",
    "    # Scrapping the rules of bivariate\n",
    "    df_association = association_rules(df1, metric = 'confidence', min_threshold =threshold)\n",
    "    \n",
    "    return df_association.sort_values(by='confidence', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Monday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Tuesday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Wednesday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Thursday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Friday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Saturday\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day( 0.01 , 0.50 , \"Sunday\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLTV - Customer Lifetime Value\n",
    "# Assess financial value of each customer\n",
    "# how much money has customer already spent at your store ; loyalty of customer\n",
    "#import modules\n",
    "import pandas as pd # for dataframes\n",
    "import matplotlib.pyplot as plt # for plotting graphs\n",
    "import seaborn as sns # for plotting graphs\n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cltv():\n",
    "    # Identification of data\n",
    "    def satislar_load():\n",
    "        dff =pd.read_excel(\"../input/turkish-market-sales-dataset-with-9000items/MarketSales.xlsx\")\n",
    "        return dff\n",
    "    # Assigning a data set\n",
    "    df = satislar_load()\n",
    "    df.head()\n",
    "\n",
    "    # Calulate total purchase\n",
    "    df['TotalPurchase'] = df['AMOUNT'] * df['PRICE']\n",
    "\n",
    "    df=df.groupby('CLIENTCODE').agg({'STARTDATE': lambda date: (date.max() - date.min()).days,\n",
    "                                            'FICHENO': lambda num: len(num),\n",
    "                                            'AMOUNT': lambda quant: quant.sum(),\n",
    "                                            'TotalPurchase': lambda price: price.sum()})\n",
    "\n",
    "    # Change the name of columns\n",
    "    df.columns=['num_days','num_transactions','num_units','spent_money']\n",
    "    df.head()\n",
    "\n",
    "\n",
    "    #1. Calculate Average Order Value\n",
    "   \n",
    "    # Average Order Value\n",
    "    df['avg_order_value']=df['spent_money']/df['num_transactions']\n",
    "\n",
    "    #2. Calculate Purchase Frequency\n",
    "\n",
    "    purchase_frequency=sum(df['num_transactions'])/df.shape[0]\n",
    "\n",
    "    #3. Calculate Repeat Rate and Churn Rate\n",
    "\n",
    "    # Repeat Rate\n",
    "    repeat_rate=df[df.num_transactions > 1].shape[0]/df.shape[0]\n",
    "\n",
    "    #Churn Rate\n",
    "    churn_rate=1-repeat_rate\n",
    "\n",
    "    # Profit Margin\n",
    "    df['profit_margin']=df['spent_money']*0.10\n",
    "\n",
    "    # Customer Value\n",
    "    df['CLV']=(df['avg_order_value']*purchase_frequency)/churn_rate\n",
    "\n",
    "    # Customer Lifetime Value\n",
    "    df['cust_lifetime_value']=df['CLV']*df['profit_margin']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cltv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
